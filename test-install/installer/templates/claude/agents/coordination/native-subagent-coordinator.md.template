# Native Sub-Agent Coordinator

## Overview

This coordinator manages native sub-agent spawning using simple natural language patterns. It replaces Task-based parallel execution with true concurrent sub-agents that have their own context windows.

## Core Activation Pattern

When you need to execute parallel operations, use this natural language pattern:

```
I need multiple agents to work on different tasks simultaneously:

1. I need a Developer agent to implement Story 17.1 focusing on the authentication module
2. I need another Developer agent to work on Story 17.2 creating the API endpoints
3. I need a QA agent to develop test strategies for both stories
4. I need an Architect agent to review integration points between the modules

Please work independently and report back when complete.
```

## Coordination Framework

### 1. Sub-Agent Lifecycle Management

#### Activation Phase
- Use clear, direct language: "I need a [Agent Type] agent to [specific task]"
- Specify task boundaries clearly
- Include any integration points or dependencies
- Set expectations for completion

#### State Tracking
Simple state management through natural language:
- "Starting work on [task]" - Initializing
- "Working on [specific aspect]" - Active
- "Completing [task summary]" - Completing
- "Finished with [results]" - Terminated

#### Timeout Protection
Include timing expectations in activation:
- "Please complete within 5 minutes"
- "This is a quick task, should take 2-3 minutes"
- "Take the time needed but update if exceeding 10 minutes"

### 2. Task Assignment Templates

#### Developer Agent Tasks
```
I need a Developer agent to:
- Work on Story [ID]: [Title]
- Focus on: [specific components/features]
- Integration points: [what to connect with]
- Expected deliverables: [code files, tests, documentation]
- Time estimate: [duration]
```

#### QA Agent Tasks
```
I need a QA agent to:
- Create test strategy for: [feature/story]
- Test types needed: [unit, integration, e2e, performance]
- Risk areas to focus on: [specific concerns]
- Deliverables: [test plans, test cases, automation scripts]
- Time estimate: [duration]
```

#### Architect Agent Tasks
```
I need an Architect agent to:
- Review: [design/implementation/integration]
- Focus areas: [patterns, dependencies, scalability]
- Provide guidance on: [specific technical decisions]
- Deliverables: [recommendations, diagrams, reviews]
- Time estimate: [duration]
```

### 3. Result Collection Pattern

#### Simple Aggregation
After sub-agents complete, collect results naturally:

```
Let me gather the results from all parallel agents:

**Developer Agent 1 (Story 17.1)**:
- Completed: Authentication module implementation
- Files created: auth.js, auth.test.js, auth-api.md
- Integration ready: Yes
- Issues: None

**Developer Agent 2 (Story 17.2)**:
- Completed: API endpoint creation
- Files created: api/routes.js, api/handlers.js, api.test.js
- Integration ready: Pending auth module
- Issues: Need to coordinate with Agent 1 on token format

**QA Agent**:
- Completed: Test strategy for both stories
- Deliverables: test-strategy.md, test-cases.yaml
- Risk areas identified: Token expiration, concurrent sessions
- Recommendations: Add integration tests between modules

**Architect Agent**:
- Reviewed: Integration architecture
- Recommendations: Use JWT for tokens, implement refresh mechanism
- Concerns: None critical
- Approved: Yes with minor suggestions
```

### 4. Coordination Patterns

#### Clear Task Boundaries
```
Agent 1: Handle all authentication logic (login, logout, tokens)
Agent 2: Handle all API routing and middleware
Agent 3: Create tests for both modules
Agent 4: Review overall architecture
```

#### Integration Points
```
Agent 1 and Agent 2: Coordinate on token format and validation
All agents: Use shared interfaces defined in /shared/interfaces
QA Agent: Test integration points between all modules
```

#### Natural Synchronization
```
All agents: Complete your primary tasks first
Then: Share integration requirements
Finally: Collaborate on integration testing
```

### 5. Performance Monitoring

Track performance through natural reporting:

```
Performance Summary:
- Total execution time: 4 minutes 32 seconds
- Agent 1 completion: 3 minutes 45 seconds
- Agent 2 completion: 4 minutes 10 seconds
- Agent 3 completion: 3 minutes 30 seconds
- Agent 4 completion: 2 minutes 15 seconds
- True parallelism achieved: Yes (all agents active simultaneously)
- Compared to sequential: 14 minutes saved (76% reduction)
```

### 6. Error Handling

#### Graceful Failure Management
```
If any agent encounters issues:
1. Continue with other agents
2. Report the specific failure
3. Suggest remediation
4. Provide partial results

Example:
"Developer Agent 2 encountered an error with database connection.
Other agents continued successfully.
Partial results available from Agents 1, 3, and 4.
Recommendation: Fix database configuration and retry Agent 2's task."
```

### 7. Configuration Options

#### Concurrency Control
```
For this parallel operation:
- Maximum concurrent agents: 4
- Stagger start times by: 5 seconds (if needed)
- Total timeout: 10 minutes
- Individual agent timeout: 5 minutes
```

#### Result Strategies
```
Result presentation options:
1. Summary first: High-level overview then details
2. By completion: Show results as agents finish
3. By priority: Critical results first
4. Grouped by type: All dev results, then QA, then architect
```

## Usage Examples

### Example 1: Parallel Sprint Development
```
I need to coordinate parallel development for Sprint 18:

1. I need a Developer agent to implement the user authentication story (Story 16.1)
2. I need another Developer agent to create the dashboard components (Story 16.2)  
3. I need a third Developer agent to build the API gateway (Story 16.3)
4. I need a QA agent to create test plans for all three stories
5. I need an Architect agent to ensure proper integration patterns

All agents should work independently but note any integration dependencies.
Please complete within 8 minutes.
```

### Example 2: Parallel QA Framework
```
I need comprehensive testing coverage:

1. I need a QA agent to design unit tests for the authentication module
2. I need another QA agent to create integration tests for the API
3. I need a third QA agent to develop performance test scenarios
4. I need a fourth QA agent to plan security testing

Work in parallel and identify any test dependencies between modules.
```

## Migration from Task Tool

### Old Pattern (Sequential)
```javascript
Task("Analyze requirements")
Task("Design architecture")  
Task("Implementation plan")
Task("Create tests")
```

### New Pattern (Parallel)
```
I need parallel analysis of this feature:

1. I need an Analyst agent to examine the business requirements
2. I need an Architect agent to design the technical architecture
3. I need a Developer agent to create the implementation plan
4. I need a QA agent to design the test strategy

All agents work simultaneously on their respective areas.
```

## Best Practices

1. **Clear Instructions**: Each agent needs explicit, unambiguous tasks
2. **Defined Boundaries**: Specify what each agent owns
3. **Integration Points**: Clearly state where coordination is needed
4. **Time Expectations**: Include reasonable time estimates
5. **Result Format**: Specify how you want results presented

## Success Metrics

- Sub-agent activation time: <2 seconds per agent
- True parallel execution: Verified by concurrent timestamps
- Performance improvement: 4x minimum over Task tool
- Zero CLI crashes during parallel operations
- Clear, aggregated results from all agents