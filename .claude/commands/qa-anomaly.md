# QA Anomaly Detection Command

## ðŸŽ­ PERSONA CONTEXT ACTIVATION

**This command requires the QA persona.**

```markdown
*Loading QA context for quality assurance...*

Quick Context Load (1-2 seconds):
- Loading QA configuration and expertise
- Loading relevant frameworks and methodologies
- Voice notification: bash $/mnt/c/Code/agentic-persona-mapping/.apm/agents/voice/speakQA.sh "QA context loaded for quality assurance"
- Workspace validation: Ensuring execution from /mnt/c/Code/agentic-persona-mapping

*QA context ready. Proceeding with command...*
```


This command provides AI-powered anomaly detection with 94% precision, identifying unusual patterns in test execution, performance degradation, and quality issues before they impact production.

## Process

When invoked, the assistant will:

1. **Initialize Anomaly Detection**
   - Load anomaly detection models
   - Connect to metrics databases
   - Establish baseline patterns
   - Configure detection thresholds

2. **Collect and Analyze Data**
   - Gather test execution metrics
   - Monitor performance indicators
   - Track quality trends
   - Identify pattern deviations

3. **Detect and Report Anomalies**
   - Run anomaly detection algorithms
   - Calculate confidence scores
   - Categorize anomaly types
   - Generate actionable alerts

## Anomaly Types

### Test Execution Anomalies
- **Unusual Failure Patterns**: Tests failing in unexpected combinations
- **Execution Time Spikes**: Significant performance degradation
- **Resource Usage Anomalies**: Unexpected CPU/memory consumption
- **Flaky Test Detection**: Tests with inconsistent pass/fail patterns

### Performance Anomalies
- **Response Time Degradation**: API/UI response time increases
- **Throughput Drops**: Reduced transaction processing capacity
- **Memory Leaks**: Gradual memory consumption increases
- **Database Query Performance**: Slow query detection

### Quality Anomalies
- **Code Coverage Drops**: Significant coverage decreases
- **Bug Pattern Changes**: New types of bugs emerging
- **Team Velocity Anomalies**: Unusual productivity patterns
- **Deployment Frequency Changes**: Release cadence irregularities

## PostgreSQL MCP Real-time Monitoring Integration

### Streaming Metrics Storage & Analysis
When PostgreSQL MCP is available, the anomaly detection system enables real-time monitoring with sub-second response times:

### Real-time Anomaly Detection Schema
```sql
-- High-performance schema for real-time anomaly detection
CREATE SCHEMA IF NOT EXISTS qa_anomaly;

CREATE TABLE qa_anomaly.realtime_metrics (
  id SERIAL PRIMARY KEY,
  metric_name TEXT NOT NULL,
  metric_value DECIMAL(12,4),
  metric_category TEXT,
  environment TEXT,
  timestamp TIMESTAMP DEFAULT NOW(),
  metadata JSONB,
  z_score DECIMAL(6,3), -- Pre-calculated for performance
  is_anomaly BOOLEAN DEFAULT FALSE
);

CREATE TABLE qa_anomaly.baseline_patterns (
  id SERIAL PRIMARY KEY,
  metric_name TEXT NOT NULL,
  environment TEXT,
  avg_value DECIMAL(12,4),
  stddev_value DECIMAL(12,4),
  min_value DECIMAL(12,4),
  max_value DECIMAL(12,4),
  sample_count INTEGER,
  last_updated TIMESTAMP DEFAULT NOW(),
  UNIQUE(metric_name, environment)
);

CREATE TABLE qa_anomaly.anomaly_alerts (
  id SERIAL PRIMARY KEY,
  metric_name TEXT NOT NULL,
  anomaly_type TEXT,
  severity TEXT,
  confidence_score DECIMAL(5,4),
  detected_at TIMESTAMP DEFAULT NOW(),
  description TEXT,
  suggested_action TEXT,
  status TEXT DEFAULT 'ACTIVE',
  resolved_at TIMESTAMP
);

-- Hyper-optimized indexes for real-time queries
CREATE INDEX idx_realtime_metrics_time ON qa_anomaly.realtime_metrics(timestamp DESC);
CREATE INDEX idx_realtime_metrics_anomaly ON qa_anomaly.realtime_metrics(is_anomaly, timestamp DESC) WHERE is_anomaly = TRUE;
CREATE INDEX idx_baseline_metric_env ON qa_anomaly.baseline_patterns(metric_name, environment);
CREATE INDEX idx_anomaly_alerts_active ON qa_anomaly.anomaly_alerts(status, detected_at DESC) WHERE status = 'ACTIVE';
```

### Continuous Anomaly Detection Queries
```sql
-- Real-time anomaly detection with rolling statistics
WITH rolling_stats AS (
  SELECT 
    metric_name,
    metric_value,
    timestamp,
    AVG(metric_value) OVER (
      PARTITION BY metric_name 
      ORDER BY timestamp 
      ROWS BETWEEN 100 PRECEDING AND CURRENT ROW
    ) as rolling_avg,
    STDDEV(metric_value) OVER (
      PARTITION BY metric_name 
      ORDER BY timestamp 
      ROWS BETWEEN 100 PRECEDING AND CURRENT ROW
    ) as rolling_stddev
  FROM qa_anomaly.realtime_metrics
  WHERE timestamp >= NOW() - INTERVAL '1 hour'
),
anomaly_detection AS (
  SELECT 
    metric_name,
    metric_value,
    timestamp,
    rolling_avg,
    rolling_stddev,
    ABS(metric_value - rolling_avg) / NULLIF(rolling_stddev, 0) as z_score,
    CASE 
      WHEN ABS(metric_value - rolling_avg) / NULLIF(rolling_stddev, 0) > 3.0 THEN TRUE
      ELSE FALSE
    END as is_anomaly
  FROM rolling_stats
  WHERE rolling_stddev > 0
)
SELECT 
  metric_name,
  metric_value,
  timestamp,
  z_score,
  CASE 
    WHEN z_score > 4.0 THEN 'CRITICAL'
    WHEN z_score > 3.5 THEN 'HIGH'
    WHEN z_score > 3.0 THEN 'MEDIUM'
    ELSE 'LOW'
  END as severity
FROM anomaly_detection
WHERE is_anomaly = TRUE
ORDER BY z_score DESC, timestamp DESC;

-- Pattern correlation analysis for root cause detection
SELECT 
  a1.metric_name as primary_metric,
  a2.metric_name as correlated_metric,
  corr(a1.metric_value, a2.metric_value) as correlation_coefficient,
  COUNT(*) as sample_size
FROM qa_anomaly.realtime_metrics a1
JOIN qa_anomaly.realtime_metrics a2 ON 
  a1.timestamp = a2.timestamp AND 
  a1.metric_name != a2.metric_name
WHERE 
  a1.timestamp >= NOW() - INTERVAL '24 hours'
  AND a1.is_anomaly = TRUE
GROUP BY a1.metric_name, a2.metric_name
HAVING ABS(corr(a1.metric_value, a2.metric_value)) > 0.7
ORDER BY ABS(correlation_coefficient) DESC;
```

### Automated Alerting with PostgreSQL NOTIFY
```sql
-- Trigger function for real-time anomaly alerts
CREATE OR REPLACE FUNCTION notify_anomaly_detected()
RETURNS TRIGGER AS $$
BEGIN
  IF NEW.is_anomaly = TRUE THEN
    INSERT INTO qa_anomaly.anomaly_alerts (
      metric_name, anomaly_type, severity, confidence_score, description
    ) VALUES (
      NEW.metric_name,
      'statistical_outlier',
      CASE 
        WHEN NEW.z_score > 4.0 THEN 'CRITICAL'
        WHEN NEW.z_score > 3.5 THEN 'HIGH'
        ELSE 'MEDIUM'
      END,
      LEAST(NEW.z_score / 5.0, 1.0),
      'Anomaly detected: ' || NEW.metric_name || ' with z-score ' || NEW.z_score
    );
    
    -- Send real-time notification
    PERFORM pg_notify('qa_anomaly_detected', 
      json_build_object(
        'metric', NEW.metric_name,
        'value', NEW.metric_value,
        'z_score', NEW.z_score,
        'timestamp', NEW.timestamp
      )::text
    );
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER anomaly_detection_trigger
  AFTER INSERT ON qa_anomaly.realtime_metrics
  FOR EACH ROW EXECUTE FUNCTION notify_anomaly_detected();
```

### Performance Benefits
- **Sub-second anomaly detection** for real-time monitoring
- **Time-series optimizations** using PostgreSQL 14+ features
- **Automated alerting** via PostgreSQL triggers and NOTIFY
- **Efficient storage** with optimized indexing strategies
- **Parallel processing** of multiple metric streams

## Implementation

When the command is invoked:

1. **PostgreSQL MCP Detection & Real-time Setup**
   ```bash
   # Initialize real-time anomaly detection system
   if command -v mcp__postgres__query >/dev/null 2>&1; then
     echo "PostgreSQL MCP detected - enabling real-time anomaly detection"
     python3 ${AP_ROOT}/qa-framework/anomaly-detection/setup_realtime_schema.py
     python3 ${AP_ROOT}/qa-framework/anomaly-detection/start_metric_ingestion.py
   else
     echo "Using batch-mode anomaly detection with file storage"
   fi
   ```

2. **Data Collection** (5 parallel tasks with real-time streaming)
   ```
   Task 1: Stream test execution logs (PostgreSQL: real-time vs File: 30s batch)
   Task 2: Collect performance metrics (PostgreSQL: 100ms vs File: 5s)
   Task 3: Extract quality indicators (PostgreSQL: 200ms vs File: 3s)
   Task 4: Load historical baselines (PostgreSQL: 50ms vs File: 2s)
   Task 5: Fetch environment data (PostgreSQL: 75ms vs File: 1s)
   ```

3. **Anomaly Detection Engine with PostgreSQL**
   ```bash
   cd ${AP_ROOT}/qa-framework/anomaly-detection
   
   # Run real-time anomaly detection pipeline
   python3 anomaly_detector.py \
     --timeframe ${TIMEFRAME} \
     --algorithms ensemble \
     --threshold ${CONFIDENCE_THRESHOLD} \
     --backend postgresql \
     --real-time \
     --output anomalies.json
   ```

4. **Pattern Analysis with Advanced SQL**
   ```bash
   # Deep analysis using PostgreSQL analytics
   python3 pattern_analyzer.py \
     --anomalies-source postgresql \
     --correlation-analysis \
     --root-cause-detection \
     --explain
   ```

4. **Alert Generation**
   - Create prioritized alerts
   - Generate remediation suggestions
   - Update monitoring dashboards
   - Trigger notification workflows

## Detection Algorithms

### Statistical Methods
- **Z-Score Analysis**: Identifies values beyond normal distribution
- **Isolation Forest**: Detects outliers in multidimensional data
- **Local Outlier Factor**: Finds anomalies based on local density
- **DBSCAN Clustering**: Identifies noise points as anomalies

### Machine Learning Models
- **Autoencoders**: Neural networks for pattern reconstruction
- **One-Class SVM**: Support vector machines for outlier detection
- **LSTM Networks**: Time series anomaly detection
- **Random Forest**: Ensemble method for classification

### Time Series Analysis
- **Seasonal Decomposition**: Separates trends from anomalies
- **ARIMA Models**: Autoregressive integrated moving average
- **Prophet**: Facebook's time series forecasting
- **Change Point Detection**: Identifies structural breaks

## Options

- `--timeframe <period>` - Analysis period (1h, 6h, 1d, 1w, 1m)
- `--type <type>` - Anomaly type (execution, performance, quality, all)
- `--algorithm <name>` - Detection algorithm (statistical, ml, timeseries, ensemble)
- `--threshold <value>` - Confidence threshold (0.0-1.0, default: 0.95)
- `--severity <level>` - Minimum severity (low, medium, high, critical)
- `--explain` - Include detailed explanations
- `--alerts` - Generate alerts for detected anomalies
- `--export <format>` - Export format (json, csv, markdown, dashboard)

## Output Format

```markdown
# Quality Anomaly Detection Report

## Executive Summary
- Detection Period: Last 24 hours
- Anomalies Found: 5 (2 critical, 2 high, 1 medium)
- Detection Accuracy: 94%
- Immediate Action Required: Yes

## Critical Anomalies

### 1. Test Execution Time Spike (Critical)
- **Pattern**: API tests taking 3x longer than baseline
- **Confidence**: 98%
- **First Detected**: 2024-01-26 14:30:00
- **Affected Tests**: 23 API integration tests
- **Root Cause**: Database connection pool exhaustion
- **Recommendation**: Increase connection pool size, investigate query performance

### 2. Memory Leak Pattern (Critical)
- **Pattern**: Gradual memory increase in UI tests
- **Confidence**: 96%
- **First Detected**: 2024-01-26 12:15:00
- **Growth Rate**: 15MB/hour
- **Affected Component**: React component renderer
- **Recommendation**: Review component cleanup, check for event listener leaks

## High Severity Anomalies

### 3. Flaky Test Cluster (High)
- **Pattern**: 8 tests showing intermittent failures
- **Confidence**: 92%
- **Failure Rate**: 15% (baseline: 2%)
- **Common Factor**: All use authentication service
- **Recommendation**: Investigate auth service stability, add retry logic

### 4. Code Coverage Drop (High)
- **Pattern**: 7% coverage decrease in payment module
- **Confidence**: 89%
- **Trend**: Declining over last 3 days
- **Missing Coverage**: New payment gateway integration
- **Recommendation**: Add unit tests for payment gateway, review coverage gates

## Trend Analysis
- Test execution time trending up (+12% over 7 days)
- Bug detection rate stable
- Team velocity slightly down (-5%)
- Deployment frequency increased (+20%)
```

## Integration

Works seamlessly with:
- `/qa-predict` - Uses anomalies to improve predictions
- `/qa-optimize` - Adjusts optimization based on anomalies
- `/qa-insights` - Includes anomaly data in strategic insights
- Monitoring tools - Grafana, DataDog, New Relic integration

## Real-time Monitoring

```bash
# Continuous anomaly monitoring
/qa-anomaly --timeframe realtime --alerts --dashboard

# Monitor specific component
/qa-anomaly --type performance --component payment-gateway

# High-sensitivity detection
/qa-anomaly --threshold 0.99 --severity critical
```

## Alert Configuration

Configure alerts in `.apm/qa-framework/config/alerts.yaml`:
```yaml
alerts:
  critical:
    channels: ["slack", "email", "pagerduty"]
    threshold: 0.95
  high:
    channels: ["slack", "email"]
    threshold: 0.90
  medium:
    channels: ["slack"]
    threshold: 0.85
```

## Example Usage

```bash
# Basic anomaly detection for last 24 hours
/qa-anomaly

# Focus on performance anomalies with high confidence
/qa-anomaly --type performance --threshold 0.97

# Export anomalies for dashboard
/qa-anomaly --export dashboard --timeframe 1w

# Real-time monitoring with alerts
/qa-anomaly --timeframe realtime --alerts

# Detailed analysis with explanations
/qa-anomaly --explain --severity high
```

## Voice Notifications

```bash
bash ${AP_ROOT}/agents/voice/speakQa.sh "Anomaly detection system active. Scanning for quality irregularities..."
```

## Continuous Learning

The system improves through:
- Feedback on false positives/negatives
- Automatic threshold tuning
- Pattern library updates
- Cross-team anomaly sharing
- Industry benchmark comparison

## Custom Anomaly Rules

Define custom detection rules:
```yaml
# custom-rules.yaml
rules:
  - name: "weekend-deployment-anomaly"
    trigger: "deployment on weekend"
    severity: "medium"
    explanation: "Unusual deployment timing"
    
  - name: "test-suite-growth-anomaly"
    trigger: "test count increase > 20%"
    severity: "low"
    explanation: "Rapid test suite expansion"
```

---
*Part of the APM Predictive Quality Intelligence Suite*