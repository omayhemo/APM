# Run Tests Command

## üé≠ PERSONA CONTEXT ACTIVATION

**This command requires the QA persona.**

```markdown
*Loading QA context for quality assurance...*

Quick Context Load (1-2 seconds):
- Loading QA configuration and expertise
- Loading relevant frameworks and methodologies
- Voice notification: bash ${{SPEAK_QA}} "QA context loaded for quality assurance"
- Workspace validation: Ensuring execution from {{PROJECT_ROOT}}

*QA context ready. Proceeding with command...*
```


**Developer Agent Only**: Executes comprehensive test suites with intelligent test selection and execution optimization.

## Metadata
- **Name**: run-tests
- **Description**: Intelligent test execution with optimization and reporting
- **Agent**: Developer
- **Performance**: Optimized execution with parallel streams
- **Coverage**: Unit, integration, functional, and regression tests

## Overview

The `/qa-run-tests` command enables the Developer to execute comprehensive test suites by:
- Analyzing current code changes to determine optimal test selection
- Executing tests with intelligent parallelization and resource optimization
- Providing real-time test execution monitoring and feedback
- Generating comprehensive test reports with coverage and quality metrics
- Supporting multiple test types and execution strategies
- Integrating with CI/CD pipelines and quality gates

## Usage

```
/qa-run-tests [options]
```

## Options

- `--type <types>` - Test types to run: `unit`, `integration`, `functional`, `e2e`, `all` (default: affected)
- `--scope <scope>` - Test scope: `changed`, `affected`, `branch`, `full` (default: affected)
- `--parallel <n>` - Parallel execution streams (default: auto-detect)
- `--coverage` - Generate coverage reports (default: true)
- `--watch` - Watch mode for continuous testing (default: false)
- `--timeout <minutes>` - Global test timeout (default: 30)
- `--bail` - Stop on first failure (default: false)
- `--verbose` - Detailed output (default: false)

## Examples

```bash
# Run affected tests based on code changes
/qa-run-tests

# Run all unit and integration tests
/qa-run-tests --type unit,integration --scope full

# Run tests in watch mode for development
/qa-run-tests --watch --type unit

# Run full test suite with coverage
/qa-run-tests --type all --coverage --verbose

# Run tests for specific branch changes
/qa-run-tests --scope branch --parallel 4

# Fast feedback - bail on first failure
/qa-run-tests --type unit --bail --timeout 5
```

## Prerequisites

Before running this command, ensure:
- [ ] Test framework is configured and dependencies are installed
- [ ] Test data and fixtures are available
- [ ] Test environments are accessible and ready
- [ ] Code changes are committed or staged for affected test detection

## üöÄ INITIALIZATION PROTOCOL (MANDATORY)

**CRITICAL**: Upon activation, you MUST immediately execute parallel initialization:

```
I'm initializing the Test Execution process. Let me load all required context in parallel for optimal performance.

*Executing parallel initialization tasks:*
[Execute all 5 tasks in single function_calls block]
- Task 1: Analyze code changes to determine affected tests and optimal execution strategy
- Task 2: Load test configuration from {{PROJECT_ROOT}}/tests/test-config.json
- Task 3: Verify test environment status and dependencies
- Task 4: Load testing standards from {{AP_ROOT}}/templates/testing-standards.md
- Task 5: Check test data and fixtures availability
```

## Test Execution Workflow

### Phase 1: Test Discovery & Planning
1. **Change Analysis**: Detect code changes and affected components
2. **Test Selection**: Intelligent selection based on scope and type options
3. **Resource Planning**: Optimize parallel execution and resource allocation
4. **Environment Validation**: Verify test environment readiness
5. **Execution Strategy**: Determine optimal execution order and grouping

### Phase 2: Test Execution
6. **Parallel Test Streams**: Execute tests across multiple parallel streams
7. **Real-time Monitoring**: Track progress, failures, and resource usage
8. **Failure Handling**: Immediate failure reporting and retry logic
9. **Progress Reporting**: Continuous feedback on execution status
10. **Resource Management**: Dynamic resource allocation and optimization

### Phase 3: Results Analysis & Reporting
11. **Result Aggregation**: Collect and consolidate all test results
12. **Coverage Analysis**: Calculate and analyze code coverage metrics
13. **Report Generation**: Generate comprehensive test execution reports
14. **Quality Assessment**: Evaluate test quality and identify improvements
15. **Integration Updates**: Update CI/CD pipeline and quality dashboards

## Test Type Execution

### Unit Tests
- **Execution**: Fast, isolated tests for individual components
- **Parallelization**: High parallelization with minimal dependencies
- **Coverage Target**: 80%+ code coverage
- **Duration**: 2-5 minutes for full suite

### Integration Tests
- **Execution**: Component interaction and API testing
- **Parallelization**: Moderate parallelization with shared resources
- **Coverage Target**: Key integration points and data flows
- **Duration**: 5-10 minutes for full suite

### Functional Tests
- **Execution**: Business requirement and user story validation
- **Parallelization**: Balanced parallelization with test data management
- **Coverage Target**: All critical user workflows
- **Duration**: 8-15 minutes for full suite

### End-to-End Tests
- **Execution**: Complete user journey testing via browser automation
- **Parallelization**: Limited parallelization due to browser resources
- **Coverage Target**: Primary user workflows and critical paths
- **Duration**: 10-20 minutes for full suite

## Intelligent Test Selection

### Affected Tests (Default)
- Analyzes git changes to identify modified files
- Maps file changes to related test files and dependencies
- Selects tests that validate changed functionality
- Optimizes execution time while maintaining quality coverage

### Change-Based Selection
- Tests only for files modified in current working directory
- Minimal test execution for rapid feedback
- Ideal for development workflow and quick validation

### Branch-Based Selection
- Compares current branch with base branch (main/develop)
- Identifies all changes since branch creation
- Comprehensive testing for feature branches and pull requests

### Full Suite Execution
- Executes complete test suite regardless of changes
- Used for release validation and comprehensive quality checks
- Longest execution time but maximum confidence

## Parallel Execution Strategy

### Auto-Detection (Default)
- Automatically detects available CPU cores and memory
- Optimally allocates parallel streams based on system resources
- Balances execution speed with system stability

### Custom Parallelization
- Manual specification of parallel execution streams
- Fine-tuning for specific environments or requirements
- Override auto-detection for specialized scenarios

### Resource Optimization
- Intelligent resource allocation across test types
- Dynamic scaling based on test complexity and dependencies
- Prevents resource contention and system overload

## Output Format

```markdown
# Test Execution Results

## Execution Summary
- **Started**: [timestamp]
- **Duration**: [X] minutes [Y] seconds
- **Test Strategy**: [affected/changed/branch/full]
- **Parallel Streams**: [N] streams
- **Total Tests**: [N] tests executed

## Results Overview
‚úÖ **PASSED**: [N] tests ([X]%)
‚ùå **FAILED**: [N] tests ([X]%)
‚è≠Ô∏è **SKIPPED**: [N] tests ([X]%)
üîÑ **RETRIED**: [N] tests

## Test Type Results

### Unit Tests ‚úÖ PASSED
- **Tests**: 247 passed, 0 failed, 3 skipped
- **Duration**: 2m 34s
- **Coverage**: 84.5% (target: 80%+)
- **Parallel Streams**: 4 streams

### Integration Tests ‚úÖ PASSED  
- **Tests**: 89 passed, 0 failed, 1 skipped
- **Duration**: 4m 12s
- **Coverage**: API endpoints, database operations
- **Parallel Streams**: 3 streams

### Functional Tests ‚ö†Ô∏è PARTIAL
- **Tests**: 156 passed, 2 failed, 0 skipped
- **Duration**: 8m 45s
- **Coverage**: User workflows, business logic
- **Parallel Streams**: 2 streams

### End-to-End Tests ‚ùå FAILED
- **Tests**: 23 passed, 5 failed, 2 skipped
- **Duration**: 12m 18s (timeout: 15m)
- **Coverage**: Critical user journeys
- **Parallel Streams**: 1 stream (browser limitation)

## Failure Analysis

### Failed Tests (7 total)

#### 1. test_user_authentication_flow
**File**: tests/functional/test_auth.py:45
**Error**: AssertionError: Login redirect not working
**Duration**: 15.3s
**Retry Attempts**: 2/3
**Stack Trace**: [Truncated stack trace]

#### 2. test_payment_processing_integration  
**File**: tests/e2e/test_checkout.py:123
**Error**: TimeoutException: Payment form not responding
**Duration**: 30.0s (timeout)
**Retry Attempts**: 1/3
**Stack Trace**: [Truncated stack trace]

### Error Categories
- **Authentication Issues**: 2 tests
- **UI Responsiveness**: 3 tests  
- **API Timeouts**: 2 tests

## Coverage Report

### Overall Coverage: 79.8% (Target: 80%)
- **Lines Covered**: 15,847 / 19,856
- **Branches Covered**: 12,234 / 15,678  
- **Functions Covered**: 2,891 / 3,156

### Coverage by Module
| Module | Coverage | Lines | Branches | Status |
|--------|----------|--------|----------|--------|
| auth/ | 92.4% | 1,245/1,347 | 892/967 | ‚úÖ |
| api/ | 87.1% | 3,456/3,967 | 2,234/2,567 | ‚úÖ |
| ui/ | 45.6% | 2,134/4,678 | 1,456/3,234 | ‚ùå |
| utils/ | 96.8% | 987/1,020 | 678/701 | ‚úÖ |

### Coverage Gaps
- **Low Coverage Areas**: UI components, error handling paths
- **Missing Tests**: Edge cases, error conditions  
- **Improvement Opportunities**: Integration scenarios, async operations

## Performance Metrics

### Execution Performance
- **Total Execution Time**: 16m 42s
- **Parallel Efficiency**: 73% (vs sequential: ~45m)
- **Average Test Time**: 2.8s per test
- **Resource Usage**: CPU 68%, Memory 4.2GB

### Optimization Opportunities
- **Slow Tests**: 12 tests >30s execution time
- **Flaky Tests**: 3 tests with intermittent failures
- **Resource Bottlenecks**: Database connection pool limits

## Quality Gates

### ‚úÖ PASSED Gates
- **Unit Test Coverage**: 84.5% ‚â• 80% ‚úì
- **Critical Path Tests**: 100% passed ‚úì  
- **Build Stability**: No breaking changes ‚úì

### ‚ùå FAILED Gates
- **Overall Coverage**: 79.8% < 80% ‚ùå
- **E2E Test Success**: 82% < 90% required ‚ùå
- **Performance Regression**: 2 tests slower than baseline ‚ùå

## Next Steps

### Immediate Actions Required
1. **Fix Failed Tests**: Address 7 failing tests before deployment
2. **Increase Coverage**: Add tests for UI module to reach 80% target
3. **Performance Investigation**: Analyze slow-performing tests

### Recommended Improvements
- [ ] Add tests for low-coverage UI components
- [ ] Implement retry logic for flaky E2E tests
- [ ] Optimize database queries in slow integration tests
- [ ] Review timeout settings for E2E test suite

### CI/CD Integration
- **Deployment Status**: ‚ö†Ô∏è BLOCKED (quality gates failed)
- **Required Fixes**: 7 test failures, coverage below target
- **Estimated Fix Time**: 2-4 hours
```

## Integration Points

### Development Workflow
- **Pre-commit Testing**: Fast unit test execution
- **Pull Request Validation**: Affected test execution
- **Continuous Integration**: Full suite execution on merge
- **Release Validation**: Comprehensive testing before deployment

### Quality Assurance
- **Test Strategy Alignment**: Coordinate with `/qa-test-strategy --parallel`
- **Quality Metrics**: Feed into QA dashboard and reporting
- **Defect Tracking**: Link test failures to issue management
- **Coverage Analysis**: Support quality improvement initiatives

### DevOps Integration
- **CI/CD Pipeline**: Automated test execution triggers
- **Quality Gates**: Pass/fail criteria for deployment
- **Monitoring**: Test execution monitoring and alerting
- **Environment Management**: Test environment provisioning

## Available Capabilities

- **Multi-Framework Support**: Jest, Mocha, PyTest, JUnit, and more
- **Browser Testing**: Playwright, Selenium, Cypress integration
- **API Testing**: REST, GraphQL, and WebSocket testing
- **Database Testing**: Transaction rollback and data isolation
- **Performance Testing**: Load testing integration
- **Visual Regression**: Screenshot comparison testing
- **Accessibility Testing**: WCAG compliance validation
- **Security Testing**: SAST/DAST integration

## Success Metrics

- **Test Execution Speed**: Minimize total execution time
- **Test Reliability**: Reduce flaky test occurrences  
- **Coverage Achievement**: Meet or exceed coverage targets
- **Quality Gate Success**: Pass all defined quality criteria
- **Developer Productivity**: Fast feedback and reliable results

## Voice Notifications

```bash
bash {{AP_ROOT}}/voice/speakDeveloper.sh "Test execution initiated. Running intelligent test selection with parallel optimization..."
```

## MCP Plopdock Integration

**üö® CRITICAL: Use MCP Plopdock for Development Servers**

If tests require development servers (API servers, databases, web servers), ALWAYS use MCP Plopdock instead of direct bash commands:

```
# ‚ùå NEVER use direct bash commands:
npm run dev
python manage.py runserver  
php artisan serve

# ‚úÖ ALWAYS use MCP Plopdock:
mcp__apm-debug-host__server_start with parameters:
- command: "npm run dev"
- cwd: /path/to/project
- name: "Test API Server"
```

**Benefits for Testing**:
- **Persistent Servers**: Test servers remain running across test sessions
- **Real-time Monitoring**: Full server output visibility at http://localhost:2601
- **Clean Isolation**: Each test run gets fresh server instances
- **Automatic Cleanup**: Servers properly terminated after tests

## Advanced Configuration

```yaml
# test-execution-config.yaml
test_execution:
  defaults:
    parallel_streams: "auto"
    timeout: 30  # minutes
    coverage_target: 80
    
  test_types:
    unit:
      parallel: true
      timeout: 5
      coverage_required: true
    integration:
      parallel: true
      timeout: 10
      database_cleanup: true
    e2e:
      parallel: false
      timeout: 20
      browser_instances: 1
      
  quality_gates:
    coverage_threshold: 80
    e2e_success_rate: 90
    max_execution_time: 30
```

---
*Part of the APM High-Performance Development Infrastructure*